{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment-1 : Mohamed Alaa Ali Mahmoud , SEC2 , BN 19"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the necessary modules and setting a seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from random import Random\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure\n",
    "from math import sqrt\n",
    "SEED = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating a set of random points in the 2 dimentional space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_rand_pts(N=1000):\n",
    "    \"\"\"\n",
    "    returns 2 lists where the first one contains the x-coordinates of the points generated randomly,\n",
    "    and the second one contains the y-coordinates of the points generated randomly\n",
    "    :param N: (int, optional) default = 1000,\n",
    "        number of random generated points\n",
    "    :return: tuple of 2 lists\n",
    "    \"\"\"\n",
    "    if N<=0:\n",
    "        raise ValueError(\"Wrong value for N\")\n",
    "    rand_gen = Random(x=SEED)\n",
    "    return (\n",
    "        [rand_gen.uniform(a=0, b=1) for _ in range(N)],\n",
    "        [rand_gen.uniform(a=0, b=1) for _ in range(N)]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss(data_x, data_y, x_p, y_p):\n",
    "    \"\"\"\n",
    "takes the data set and the initial guess and returns the evaluation of our loss function\n",
    ":param data_x:(list of floats) : x-coordinate of the data point for the training data\n",
    ":param data_y:(list of floats) : y-coordinate of the data point for the training data\n",
    ":param x_p:(float) : x-coordinate of the data point for the initial guess\n",
    ":param y_p:(float) : y-coordinate of the data point for the initial guess\n",
    ":return: loss (float): The root mean squared distance between\n",
    "    the point (x_p, y_p) and the data points\n",
    "\"\"\"\n",
    "    n_inv = 1/len(data_x)\n",
    "    return n_inv * sum(\n",
    "        [ ((x_i-x_p)**2 + (y_i-y_p)**2)**0.5 for x_i, y_i in zip(data_x, data_y)]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Numeric Conceptualization of Derivatives\n",
    "It comes for the main definition of the derivative\n",
    "<br><br>\n",
    "$\\frac{\\partial \\mathbb{L}}{\\partial x} = \\lim_{h \\to 0} \\frac{\\mathbb{L}(x+h) - \\mathbb{L}(x)}{h}$\n",
    "<br><br>\n",
    "By replacing the asymptotically infinitesimal by pragmatically small of values we can get a definition of deriviative that can be numerically computed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_min(data_x:list, data_y:list, x_p=5, y_p=5, h=0.001, delta=0.01, epochs=3000):\n",
    "    \"\"\"\n",
    "    Finds the minimum loss values for x and y parameters given a list of points (x_i, y_i)\n",
    "    and a starting point (x_p, y_p) using gradient descent optimization.\n",
    "\n",
    "    Parameters:\n",
    "    data_x: list\n",
    "        A list of x values from the dataset to be used as reference points.\n",
    "    data_y: list\n",
    "        The corresponding list of y values from the dataset to be used as reference points.\n",
    "    x_p: float, default=5\n",
    "        The starting value for the x parameter in the optimization.\n",
    "    y_p: float, default=5\n",
    "        The starting value for the y parameter in the optimization.\n",
    "    h: float, default=0.001\n",
    "        The step size for approximating the gradient.\n",
    "    delta: float, default=0.01\n",
    "        The learning rate for the gradient descent algorithm.\n",
    "    epochs: int\n",
    "        The number of times to iterate through the optimization algorithm.\n",
    "    Returns:\n",
    "    --------\n",
    "    lst_xp: list of floats\n",
    "        The optimized values for the x parameter.\n",
    "    lst_yp: list of floats\n",
    "        The optimized values for the y parameter.\n",
    "    epoch_losses: list of floats\n",
    "        A list of the loss values for each epoch during optimization.\n",
    "    \"\"\"\n",
    "    epoch_losses=[]\n",
    "    lst_xp = []\n",
    "    lst_yp= []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        epoch_losses.append(loss(data_x=data_x,data_y=data_y, x_p=x_p,y_p=y_p))\n",
    "        lst_xp.append(x_p)\n",
    "        lst_yp.append(y_p)\n",
    "\n",
    "        dloss_dx = (loss(data_x, data_y, x_p + h, y_p) - loss(data_x, data_y, x_p, y_p)) / h\n",
    "        dloss_dy = (loss(data_x, data_y, x_p, y_p + h) - loss(data_x, data_y, x_p, y_p)) / h\n",
    "\n",
    "        x_p -= delta * dloss_dx\n",
    "        y_p -= delta * dloss_dy\n",
    "\n",
    "    return epoch_losses, lst_xp, lst_yp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyper Params Tuning\n",
    "Here we tune the following hyper params to find the best fit for our data <br>\n",
    "<ul>\n",
    "    first case : xp , yp = negative <br>\n",
    "    second case: xp, yp ~= 0.5, 0.5 <br>\n",
    "    third case : h large number <br>\n",
    "    fourth case : delta large number <br>\n",
    "    fifth case : delta small number <br>\n",
    "    Epochs always 3000\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_case(data_x:list, data_y:list, x_p=5, y_p=5, h=0.001, delta=0.01, epochs=3000):\n",
    "    \"\"\"\n",
    "    gets the results for the case passedd\n",
    "    \"\"\"\n",
    "    return find_min(data_x, data_y, x_p, y_p, h, delta, epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_x , data_y = gen_rand_pts()\n",
    "xp = [-5, 0.48]\n",
    "yp = [-4, 0.48]\n",
    "h = 100\n",
    "delta = [100, 0.00001]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "losses1,xp_1,yp_1 = find_case(data_x=data_x, data_y=data_y, x_p=xp[0], y_p=yp[0])\n",
    "losses2,xp_2,yp_2 = find_case(data_x=data_x, data_y=data_y, x_p=xp[1], y_p=yp[1])\n",
    "losses3,xp_3,yp_3 = find_case(data_x=data_x, data_y=data_y, h=100)\n",
    "losses4,xp_4,yp_4 = find_case(data_x=data_x, data_y=data_y, delta=delta[0])\n",
    "losses5,xp_5,yp_5 = find_case(data_x=data_x, data_y=data_y, delta=delta[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlim(left = -1, right = 2)\n",
    "ax.set_ylim(top = -1, bottom = 2)\n",
    "ax.scatter(data_x, data_y)\n",
    "ax.set_title(\"Visualizing the data points\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the first case x_p, y_p are negatives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig2 = figure(figsize=(20,10))\n",
    "ax2=fig2.add_subplot(1,2,1)\n",
    "ax2.plot(losses1)\n",
    "ax2.set_title(\"Loss versus Epochs Case 1\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.text(800, 4,\n",
    "             'We Notice that the curve of \\n'\n",
    "             'losses is decreasing with the epochs number\\n'\n",
    "             'asymptotically,\\nwhere the value of the loss gets\\n nearly constant around the 560 Epoch',\n",
    "             style = 'italic',\n",
    "             fontsize = 15,\n",
    "             color = \"green\")\n",
    "ax2.hlines(np.array(losses1).min()-0.01, xmin=0, xmax=3000, colors=\"r\", linestyles=\"--\")\n",
    "\n",
    "ax3 = fig2.add_subplot(1,2,2)\n",
    "ax3.plot(xp_1, label=\"x_p\")\n",
    "ax3.plot(yp_1, label=\"y_p\")\n",
    "ax3.legend()\n",
    "ax3.set_title(\"X_P, Y_P where they are initialized negative values at the beginning versus Epochs Case 1\")\n",
    "ax3.set_xlabel(\"Epochs\")\n",
    "ax3.set_ylabel(\"X_P, Y_P\")\n",
    "ax3.text(700, -4,\n",
    "         'We Notice that the curve of \\n'\n",
    "         'X_P and Y_P is increasing with the epochs number\\n'\n",
    "         'asymptotically\\n,where the value of the X_P,Y_P gets\\n nearly constant around the 560 Epoch\\n'\n",
    "         'and they overlap each other\\n due to the symmetry of our problem',\n",
    "         style = 'italic',\n",
    "         fontsize = 15,\n",
    "         color = \"green\")\n",
    "ax3.hlines(np.array(xp_1).max()+0.01, xmin=0, xmax=3000, colors=\"r\", linestyles=\"--\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the second case x_p, y_p are close to the ground truth"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_case2 = figure(figsize=(20,10))\n",
    "ax_case2 = fig_case2.add_subplot(1,2,1)\n",
    "ax_case2.plot(losses2)\n",
    "ax_case2.set_title(\"Loss versus Epochs Case 2\")\n",
    "ax_case2.set_xlabel(\"Epochs\")\n",
    "ax_case2.set_ylabel(\"Loss\")\n",
    "ax_case2.hlines(np.array(losses2).min()-0.000001, xmin=0, xmax=3000, colors=\"r\", linestyles=\"--\")\n",
    "\n",
    "ax_case2_2 = fig_case2.add_subplot(1,2,2)\n",
    "ax_case2_2.plot(xp_2, label=\"x_p\")\n",
    "ax_case2_2.plot(yp_2, label=\"y_p\")\n",
    "ax_case2_2.legend()\n",
    "ax_case2_2.set_title(\"X_P, Y_P where they are initialized close to the ground truth values versus Epochs Case 2\")\n",
    "ax_case2_2.set_xlabel(\"Epochs\")\n",
    "ax_case2_2.set_ylabel(\"X_P, Y_P\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here for X_P and Y_P the curves show there will always be a difference between them about 0.1 but we are still close to our best solution <br>\n",
    "We also notice that the loss dropped very fast and this is predictable since we started from a guess which was very close to the grand truth <br>\n",
    "so we converged in the first 300 epoch which is faster than the first time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing Case 3 : h is set to a large number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_case3 = figure(figsize=(20,10))\n",
    "ax_case3 = fig_case3.add_subplot(1,2,1)\n",
    "ax_case3.plot(losses3)\n",
    "ax_case3.set_title(\"Loss versus Epochs Case 3\")\n",
    "ax_case3.set_xlabel(\"Epochs\")\n",
    "ax_case3.set_ylabel(\"Loss\")\n",
    "ax_case3.hlines(np.array(losses3).min()-0.000001, xmin=0, xmax=3000, colors=\"r\", linestyles=\"--\")\n",
    "\n",
    "ax_case3_2 = fig_case3.add_subplot(1,2,2)\n",
    "ax_case3_2.plot(xp_3, label=\"x_p\")\n",
    "ax_case3_2.plot(yp_3, label=\"y_p\")\n",
    "ax_case3_2.legend()\n",
    "ax_case3_2.set_title(\"X_P, Y_P where h is initialized to a large number versus Epochs Case 3\")\n",
    "ax_case3_2.set_xlabel(\"Epochs\")\n",
    "ax_case3_2.set_ylabel(\"X_P, Y_P\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a total mess due to the following reasons:<br>\n",
    "    - In gradient descent, the step size of the update to the parameters is determined by the derivative of the loss function with respect to the parameters.\n",
    "    - Since we set h to a very large number this means we are making the resulting values for the derivatives very small or even zero\n",
    "    - so we are updating with a very small step, so we do not converge or even failing in convergence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## visualizing the fourth case DELTA is very large"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_case4 = figure(figsize=(20,10))\n",
    "ax_case4 = fig_case4.add_subplot(1,2,1)\n",
    "ax_case4.plot(losses4)\n",
    "ax_case4.set_title(\"Loss versus Epochs Case 4\")\n",
    "ax_case4.set_xlabel(\"Epochs\")\n",
    "ax_case4.set_ylabel(\"Loss\")\n",
    "ax_case4.hlines(np.array(losses4).min()-0.000001, xmin=0, xmax=3000, colors=\"r\", linestyles=\"--\")\n",
    "\n",
    "ax_case4_2 = fig_case4.add_subplot(1,2,2)\n",
    "ax_case4_2.plot(xp_4, label=\"x_p\")\n",
    "ax_case4_2.plot(yp_4, label=\"y_p\")\n",
    "ax_case4_2.legend()\n",
    "ax_case4_2.set_title(\"X_P, Y_P where delta is initialized to a large number versus Epochs Case 4\")\n",
    "ax_case4_2.set_xlabel(\"Epochs\")\n",
    "ax_case4_2.set_ylabel(\"X_P, Y_P\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again Very Messy Curves and also this was predictable<br>\n",
    "    -Divergence in the loss curve: When the learning rate is too high, the weights can change too much with each update, causing the loss function to increase instead of decreasing. This results in the model's predictions becoming worse and worse over time, rather than improving.\n",
    "    - This is called the problem of exploding gradients"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing the fifth case delta small number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_case5 = figure(figsize=(20,10))\n",
    "ax_case5 = fig_case5.add_subplot(1,2,1)\n",
    "ax_case5.plot(losses5)\n",
    "ax_case5.set_title(\"Loss versus Epochs Case 5\")\n",
    "ax_case5.set_xlabel(\"Epochs\")\n",
    "ax_case5.set_ylabel(\"Loss\")\n",
    "ax_case5.hlines(np.array(losses5).min()-0.000001, xmin=0, xmax=3000, colors=\"r\", linestyles=\"--\")\n",
    "\n",
    "ax_case5_2 = fig_case5.add_subplot(1,2,2)\n",
    "ax_case5_2.plot(xp_5, label=\"x_p\")\n",
    "ax_case5_2.plot(yp_5, label=\"y_p\")\n",
    "ax_case5_2.legend()\n",
    "ax_case5_2.set_title(\"X_P, Y_P where delta is initialized to a small number versus Epochs Case 5\")\n",
    "ax_case5_2.set_xlabel(\"Epochs\")\n",
    "ax_case5_2.set_ylabel(\"X_P, Y_P\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Slow convergence: When the learning rate is too small, the weights are updated very slowly, which can lead to slow convergence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the previous analysis we found that : <br>\n",
    "    -Setting the learning rate to a large number can cause **exploding gradients** problem <br>\n",
    "    -Setting the learning rate to a very small number can cause the algorithm to take very long time to converge <br>\n",
    "    -Setting the value of H to a very large number will lead to the **vanishing gradients** problem<br>\n",
    "    -initialization can play a good role in making the learning algorithm converge faster <br>\n",
    "\n",
    "So , its always the best to tune the hyperparameters using reasonable numbers to save time and effort since searching in a wrong direction can\n",
    "cost a lot of time and effort<br>\n",
    "Many searching techniques can be used to find the best combinations of parameters for the machine learning model such as :\n",
    "**GridSearching** and **RandomizedSearching** to find the best params\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
